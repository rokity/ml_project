Questo report descrive l’implementazione in Python di una Feedforward Neural Network che offre la possibilità di utilizzare diversi algoritmi di ottimizzazione. Abbiamo implementato da zero gli ottimizzatori RMSprop, Adam e SGD utilizzando in quest'ultimo la strategia della linear learning rate decay. La rete neurale è stata testata su task supervisionati di classificazione (Monk) e di regressione (Cup).
